{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Annotation => filename ; y ; x ; x_min ; y_min ; x_max ; y_max ; category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 256)  # To resize all images to 64x64\n",
    "DATASET_PATH = \"TSRD_Train\"  # Path to your dataset folder\n",
    "ANNOTATION_FILE = \"TSRD_Train_Annotation/TsignRecgTrain4170Annotation.txt\"  # Path to the annotation file\n",
    "SAVE_PATH = \"processed_data.npz\"  # File to save the processed dataset (NPZ to store NumPy arrays)\n",
    "TEST_DATASET_PATH = \"images\"  # Path to your test dataset folder\n",
    "TEST_ANNOTATION_FILE = \"TSRD_Test_Annotation/TsignRecgTest1994Annotation.txt\"  # Path to the test annotation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse annotation file #####################################################\n",
    "def parse_annotations(annotation_file):\n",
    "    annotations = []\n",
    "    with open(annotation_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\";\")\n",
    "            filename = parts[0]\n",
    "            y_axis = int(parts[1])\n",
    "            x_axis = int(parts[2])\n",
    "            bound_x_min = int(parts[3])\n",
    "            bound_y_min = int(parts[4])\n",
    "            bound_x_max = int(parts[5])\n",
    "            bound_y_max = int(parts[6])\n",
    "            label = int(parts[7])\n",
    "            annotations.append({\n",
    "                \"filename\": filename,\n",
    "                \"y_axis\": y_axis,\n",
    "                \"x_axis\": x_axis,\n",
    "                \"bound_x_min\": bound_x_min,\n",
    "                \"bound_y_min\": bound_y_min,\n",
    "                \"bound_x_max\": bound_x_max,\n",
    "                \"bound_y_max\": bound_y_max,\n",
    "                \"label\": label\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Function to load images ################################################\n",
    "def load_images(image_folder, annotations):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for ann in annotations:\n",
    "        # Get the image path\n",
    "        img_path = os.path.join(image_folder, ann[\"filename\"])\n",
    "        # Read the image\n",
    "        img = io.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "            continue\n",
    "        images.append(img)\n",
    "        labels.append(ann[\"label\"])\n",
    "    return images, labels\n",
    "\n",
    "# get the acutal labels of the test dataset\n",
    "def get_actual_labels(TEST_DATASET_PATH):\n",
    "    actual_labels = []\n",
    "    for filename in os.listdir(TEST_DATASET_PATH):\n",
    "        label = int(filename.split(\"_\")[0])\n",
    "        actual_labels.append(label)\n",
    "    return actual_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load and preprocess images ################################################\n",
    "def preprocess_images(image_folder, annotations, image_size):\n",
    "\n",
    "    # load images and labels\n",
    "    images, labels = load_images(image_folder, annotations)\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "    # for i in range(10):\n",
    "    \n",
    "        ann = annotations[i]\n",
    "        img = images[i]\n",
    "\n",
    "        # Crop the bounding box\n",
    "        cropped_img = img[ann[\"bound_y_min\"]:ann[\"bound_y_max\"], ann[\"bound_x_min\"]:ann[\"bound_x_max\"]]\n",
    "\n",
    "        # Convert to grayscale\n",
    "        # gray_img = (rgb2gray(cropped_img)*255).astype(np.uint8)\n",
    "        # print (gray_img)\n",
    "\n",
    "        # histogram equalization\n",
    "        # hist_eq_img = histogram_eq(gray_img)\n",
    "        # show_images([gray_img, hist_eq_img], [\"Original\", \"Histogram Equalized\"])\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = cv.resize(cropped_img, image_size, interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "        # crop the circluar region with center equals the center of the bounding box and radius equals the half of the bounding box width\n",
    "        mask1 = np.zeros_like(resized_img)\n",
    "        center = (int(resized_img.shape[1]/2), int(resized_img.shape[0]/2))\n",
    "        radius = int(resized_img.shape[1]/2)\n",
    "        mask1 = cv.circle(mask1, center, radius, (255, 255, 255), -1)\n",
    "        masked_img = cv.bitwise_and(resized_img, mask1)\n",
    "        # show_images([img, resized_img ,  masked_img], [\"Original\", \"Resized\" , \"Masked\"])\n",
    "\n",
    "        # Normalize pixel values\n",
    "        # normalized_img = masked_img / 255.0\n",
    "        normalized_img = masked_img\n",
    "\n",
    "        # show_images([img, cropped_img, resized_img, normalized_img], [\"Original\", \"Cropped\", \"Resized\", \"Normalized\"])\n",
    "        # show_images([img, resized_img, normalized_img], [\"Original\", \"Resized\", \"Normalized\"])\n",
    "\n",
    "        processed_images.append(normalized_img)\n",
    "\n",
    "    # # Convert to NumPy arrays\n",
    "    processed_images = np.array(processed_images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return processed_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape-Based Features (edge detection, Hough transform)\n",
    "\n",
    "#canny edge detection\n",
    "def canny_edge_detection(image):\n",
    "    edges = cv.Canny((image * 255).astype(np.uint8), 40, 80)\n",
    "    return edges\n",
    "\n",
    "# Lines Hough transform\n",
    "def lines_hough_transform(image):\n",
    "    gray = rgb2gray(image)\n",
    "    edges = canny_edge_detection(gray)\n",
    "    lines = cv.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=45, minLineLength=20, maxLineGap=10)\n",
    "    return lines\n",
    "\n",
    "# Circles Hough transform\n",
    "def circles_hough_transform(image):\n",
    "    gray = rgb2gray(image)\n",
    "    edges = canny_edge_detection(gray)\n",
    "    circles = cv.HoughCircles(edges, cv.HOUGH_GRADIENT, dp=1.5, minDist=300, param1=50, param2=30, minRadius=110, maxRadius=200)\n",
    "    return circles\n",
    "\n",
    "# HOG\n",
    "def extract_hog_features(image):\n",
    "    # gray = rgb2gray(image)\n",
    "    hog_features = hog(image, \n",
    "                       orientations=9, \n",
    "                       pixels_per_cell=(8, 8), \n",
    "                       cells_per_block=(2, 2), \n",
    "                       block_norm='L2-Hys', \n",
    "                       visualize=False, \n",
    "                       feature_vector=True)\n",
    "    return hog_features\n",
    "\n",
    "# Keypoint-Based Features\n",
    "\n",
    "# Harris corner detection\n",
    "def harris_corner_detection(image):\n",
    "    gray = rgb2gray(image)\n",
    "    gray = np.float32(gray)\n",
    "    dst = cv.cornerHarris(gray, blockSize=2, ksize=3, k=0.04)\n",
    "    return dst\n",
    "\n",
    "# SIFT \n",
    "def sift_features(image):\n",
    "    # image = np.uint8(image * 255)\n",
    "    # gray = rgb2gray(image)\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "# color-based features\n",
    "\n",
    "# Color histogram\n",
    "def color_histogram(image, bins=(8, 8, 8)):\n",
    "    hist = cv.calcHist([image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    return cv.normalize(hist, hist).flatten()\n",
    "\n",
    "\n",
    "# Extracting Combined Features\n",
    "def extract_combined_features(image, sift_features):\n",
    "    \n",
    "    # Convert to grayscale for SIFT and HOG\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # # SIFT features\n",
    "    # sift = cv.SIFT_create()\n",
    "    # keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    # sift_features = descriptors.flatten() if descriptors is not None else np.zeros(128)\n",
    "\n",
    "    # HOG features\n",
    "    hog_features = extract_hog_features(gray_image)\n",
    "\n",
    "    # Color histogram features\n",
    "    color_histogram_features = color_histogram(image)\n",
    "\n",
    "    # Combine features into a single vector\n",
    "    combined_features = np.hstack([sift_features, hog_features, color_histogram_features])\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classificaion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# # create vocabulary using KMeans clustering\n",
    "# def create_vocabulary(descriptors, k):\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(descriptors)\n",
    "#     return kmeans\n",
    "\n",
    "# generate BoVW histograms\n",
    "def generate_bovw_histograms(descriptors_list, kmeans, k):\n",
    "    histograms = []\n",
    "    for descriptors in descriptors_list:\n",
    "        if descriptors is not None:\n",
    "            words = kmeans.predict(descriptors)\n",
    "            histogram, _ = np.histogram(words, bins=np.arange(k+1), density=True)\n",
    "        else:\n",
    "            histogram = np.zeros(k)\n",
    "        histograms.append(histogram)\n",
    "    return np.array(histograms)\n",
    "\n",
    "def create_vocabulary(descriptors_list, num_clusters=100):\n",
    "    # Stack all descriptors into one array\n",
    "    all_descriptors = np.vstack([desc for desc in descriptors_list if desc is not None])\n",
    "    \n",
    "    # Cluster descriptors into visual words\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(all_descriptors)\n",
    "    return kmeans\n",
    "\n",
    "def extract_bow_features(image, kmeans):\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    sift = cv.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "    \n",
    "    if descriptors is None:\n",
    "        return np.zeros(kmeans.n_clusters)\n",
    "    \n",
    "    # Build histogram of visual words\n",
    "    words = kmeans.predict(descriptors)\n",
    "    histogram, _ = np.histogram(words, bins=np.arange(kmeans.n_clusters+1), density=True)\n",
    "\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train ():\n",
    "    \n",
    "    # Parse annotations\n",
    "    annotations = parse_annotations(ANNOTATION_FILE)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images, labels = preprocess_images(DATASET_PATH, annotations, IMAGE_SIZE)\n",
    "\n",
    "    print(\"Succesfully preprocessed images\")\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    #SIFT\n",
    "    descriptors_list = [sift_features(img)[1] for img in images]\n",
    "    kmeans = create_vocabulary(descriptors_list, num_clusters=100)\n",
    "\n",
    "    # Extract BoVW Features for each image\n",
    "    features = []\n",
    "    feature_labels = []\n",
    "\n",
    "    # # descreptors list for all images\n",
    "    # descriptors_list = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        bow_features = extract_bow_features(img, kmeans)\n",
    "        combined_features = extract_combined_features(img, bow_features)\n",
    "        features.append(combined_features)\n",
    "        feature_labels.append(label)\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        # edges = canny_edge_detection(img)\n",
    "\n",
    "        # # Apply Hough transform for lines\n",
    "        # lines = lines_hough_transform(img)\n",
    "        # if lines is not None:\n",
    "        #     for line in lines:\n",
    "        #         x1, y1, x2, y2 = line[0]\n",
    "        #         cv.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        # # Apply Hough transform for circles\n",
    "        # detected_circles = circles_hough_transform(img)\n",
    "\n",
    "        # if detected_circles is not None: \n",
    "        #     print (\"detected_circles\", detected_circles)\n",
    "\n",
    "        #     # Convert the circle parameters a, b and r to integers. \n",
    "        #     detected_circles = np.uint16(np.around(detected_circles)) \n",
    "\n",
    "        #     for pt in detected_circles[0, :]: \n",
    "        #         a, b, r = pt[0], pt[1], pt[2] \n",
    "\n",
    "        #         # Draw the circumference of the circle. \n",
    "        #         cv.circle(img, (a, b), r, (0, 255, 0), 2) \n",
    "\n",
    "        #         # Draw a small circle (of radius 1) to show the center. \n",
    "        #         cv.circle(img, (a, b), 1, (0, 0, 255), 3) \n",
    "\n",
    "        # # apply Harris corner detection\n",
    "        # dst = harris_corner_detection(img)\n",
    "        # dst = cv.dilate(dst, None)\n",
    "        # img[dst > 0.01 * dst.max()] = [0, 255, 0]\n",
    "\n",
    "        # print (\"img\", img * 255)\n",
    "\n",
    "        # apply SIFT\n",
    "        # keypoints, descriptors = sift_features(img)\n",
    "        # if descriptors is not None:\n",
    "        #     descriptors_list.append(descriptors)\n",
    "\n",
    "\n",
    "        # img=cv.drawKeypoints(img, keypoints,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        # cv.imwrite('sift_keypoints.jpg',img)\n",
    "        # show_images([img], [\"SIFT\"])\n",
    "\n",
    "        # show_images([img, edges], [\"Original\", \"Edges\"])\n",
    "        # print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "    # stack all descriptors\n",
    "    # stack_descriptors = np.vstack(descriptors_list)\n",
    "\n",
    "    # # create vocabulary using KMeans clustering\n",
    "    # k = 290 # number of clusters (total number of keypoints / number of dataset images * number of classes) (Average)\n",
    "    # kmeans = create_vocabulary(stack_descriptors, k)\n",
    "\n",
    "    # print(\"descriptors_list_dimentions\", len(descriptors_list))\n",
    "    # print (\"stack_descriptors_shape\", stack_descriptors.shape)\n",
    "    # print (\"kmeans\", kmeans)\n",
    "\n",
    "    # pca = PCA(n_components=2)\n",
    "    # reduced_data = pca.fit_transform(descriptors)\n",
    "\n",
    "    # # Now separate the data, Note the flatten()\n",
    "    # A = stack_descriptors[labels.ravel()==0]\n",
    "    # B = stack_descriptors[labels.ravel()==1]\n",
    "\n",
    "    # # Plot the data\n",
    "    # plt.scatter(A[:,0],A[:,1])\n",
    "    # plt.scatter(B[:,0],B[:,1],c = 'r')\n",
    "    # plt.scatter(centers[:,0],centers[:,1],s = 80,c = 'y', marker = 's')\n",
    "    # plt.xlabel('Height'),plt.ylabel('Weight')\n",
    "    # plt.show()\n",
    "\n",
    "    # # Generate BoVW Histograms for Each Image\n",
    "    # print(\"Generating BoVW histograms...\")\n",
    "    # images_histograms = generate_bovw_histograms(descriptors_list, kmeans, k)\n",
    "    # print (\"images_histograms_shape\", images_histograms.shape)\n",
    "\n",
    "    # standardize the features\n",
    "    # images_histograms = StandardScaler().fit_transform(images_histograms)\n",
    "\n",
    "    # Train SVM classifier\n",
    "    classifier = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced'))\n",
    "    # classifier = SVC(kernel='rbf', class_weight='balanced')\n",
    "    classifier.fit(features, feature_labels)\n",
    "\n",
    "    return classifier, kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (classifier, kmeans):\n",
    "\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # try for the the blind test set folder\n",
    "    # Load images\n",
    "    blind_test_images = []\n",
    "    for filename in os.listdir(TEST_DATASET_PATH):\n",
    "        img_path = os.path.join(TEST_DATASET_PATH, filename)\n",
    "        img = io.imread(img_path)\n",
    "        blind_test_images.append(img)\n",
    "\n",
    "    # # Preprocess images\n",
    "    # blind_test_images = np.array(blind_test_images)\n",
    "    # blind_test_images = blind_test_images / 255.0\n",
    "\n",
    "    # Extract SIFT features\n",
    "    blind_test_descriptors_list = []\n",
    "    for img in blind_test_images:\n",
    "        # keypoints, descriptors = sift_features(img)\n",
    "        # blind_test_descriptors_list.append(descriptors)\n",
    "\n",
    "        # img=cv.drawKeypoints(img, keypoints,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        # cv.imwrite('sift_keypoints.jpg',img)\n",
    "        # show_images([img], [\"SIFT\"])\n",
    "\n",
    "        # Handle edge case where no descriptors are found\n",
    "        # if descriptors is None:\n",
    "        #     descriptors = np.zeros((1, 128), dtype=np.float32)\n",
    "\n",
    "        # # Assign descriptors to the nearest cluster center (for each keypoint in the descriptor, find the nearest cluster center (nears word))\n",
    "        # words = kmeans.predict(descriptors)\n",
    "\n",
    "        # # Build histogram of visual words\n",
    "        # histogram = np.zeros(kmeans.n_clusters, dtype=np.float32)\n",
    "        # for word in words:\n",
    "        #     histogram[word] += 1\n",
    "\n",
    "        # Normalize histogram\n",
    "        # histogram /= len(words)\n",
    "        # print (\"histogram\", histogram)\n",
    "\n",
    "\n",
    "        bow_features = extract_bow_features(img, kmeans)\n",
    "        # print (\"bow_features_shape\", bow_features.shape)\n",
    "        # print (\"bow_features\", bow_features)\n",
    "        combined_features = extract_combined_features(img, bow_features)\n",
    "        # print (\"combined_features_shape\", combined_features.shape)\n",
    "        # Ensure the feature vector has the correct size\n",
    "        expected_size = classifier.steps[-1][1].n_features_in_\n",
    "        current_size = combined_features.shape[0]\n",
    "        \n",
    "        if current_size < expected_size:\n",
    "            # Pad with zeros if smaller\n",
    "            # print(f\"Padding: Current size {current_size}, Expected size {expected_size}\")\n",
    "            combined_features = np.pad(\n",
    "                combined_features,\n",
    "                (0, expected_size - current_size),\n",
    "                mode='constant'\n",
    "            )\n",
    "        elif current_size > expected_size:\n",
    "            # Truncate if larger\n",
    "            # print(f\"Truncating: Current size {current_size}, Expected size {expected_size}\")\n",
    "            combined_features = combined_features[:expected_size]\n",
    "\n",
    "        # predict the label\n",
    "        predicted_label = classifier.predict(combined_features.reshape(1, -1))\n",
    "        predicted_labels.append(predicted_label[0])\n",
    "        # print(f\"Predicted label: {predicted_label[0]}\")\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully preprocessed images\n",
      "Extracting features...\n"
     ]
    }
   ],
   "source": [
    "classifier, Kmeans = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main (predict test images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # predict the labels of the test dataset\n",
    "    predicted_labels = predict(classifier, Kmeans)\n",
    "\n",
    "    # get the acutal labels\n",
    "    actual_labels = get_actual_labels(TEST_DATASET_PATH)\n",
    "\n",
    "    # Evaluate Classifier\n",
    "    print (\"actual_labels   \", actual_labels)\n",
    "    print (\"predicted_labels\", predicted_labels)\n",
    "\n",
    "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(classification_report(actual_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
