{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: ./TSRD_Train\\024_1_0007_1_j.png\n",
      "Removed: ./TSRD_Train\\022_1_0007_1_j.png\n",
      "Removed: ./TSRD_Train\\026_1_0068_1_j.png\n",
      "Removed: ./TSRD_Train\\055_1_0030_1_j.png\n",
      "Completed removing unlabeled images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_images_without_labels(images_folder, labels_file):\n",
    "    # Read the labels file and extract image names\n",
    "    with open(labels_file, 'r') as file:\n",
    "        labeled_images = set(line.split(';')[0] for line in file.readlines())\n",
    "\n",
    "    # List all images in the images folder\n",
    "    all_images = set(os.listdir(images_folder))\n",
    "\n",
    "    # Identify images without labels\n",
    "    images_without_labels = all_images - labeled_images\n",
    "\n",
    "    # Remove images without labels\n",
    "    for img in images_without_labels:\n",
    "        img_path = os.path.join(images_folder, img)\n",
    "        if os.path.isfile(img_path):\n",
    "            os.remove(img_path)\n",
    "            print(f\"Removed: {img_path}\")\n",
    "    print(\"Completed removing unlabeled images.\")\n",
    "\n",
    "# Example usage\n",
    "images_folder = \"./TSRD_Train\"  # Replace with your image folder path\n",
    "labels_file = \"./TSRD_Train_Annotation/TsignRecgTrain4170Annotation.txt\"       # Replace with your .txt file path\n",
    "remove_images_without_labels(images_folder, labels_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "from transformers import DetrImageProcessor\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0    1    2   3   4    5    6   7\n",
      "0    022_0001.png  210  197  24  24  189  186  22\n",
      "1    022_0002.png   93   85  12  12   79   77  22\n",
      "2    022_0003.png  200  196  28  20  190  182  22\n",
      "3    022_0004.png  179  173  30  27  159  156  22\n",
      "4  022_0001_j.png  166  159  38  32  125  123  22\n",
      "Dataset({\n",
      "    features: ['image_path', 'bbox', 'category'],\n",
      "    num_rows: 2047\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load and parse annotations\n",
    "def load_annotations(image_folder, annotation_file):\n",
    "    # Read annotation file with semicolon delimiter\n",
    "\n",
    "    df = pd.read_csv(annotation_file, header=None, delimiter=\";\")\n",
    "    df = df.iloc[:, :-1]\n",
    "\n",
    "    # Print the first few rows to debug\n",
    "    print(df.head())\n",
    "    \n",
    "    # Assign column names based on your annotation format\n",
    "    df.columns = ['filename', 'y', 'x', 'x_min', 'y_min', 'x_max', 'y_max', 'category']\n",
    "    \n",
    "    # Prepare records for the dataset\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        record = {\n",
    "            'image_path': f\"{image_folder}/{row['filename']}\",\n",
    "            'bbox': [row['x_min'], row['y_min'], row['x_max'], row['y_max']],  # Bounding box [x_min, y_min, x_max, y_max]\n",
    "            'category': row['category']\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Example usage\n",
    "image_folder = './TSRD_Train'  # Replace with your image folder path\n",
    "annotation_file = './TSRD_Train_Annotation/TsignRecgTrain4170Annotation.txt'  # Replace with your annotation file path\n",
    "\n",
    "# Load annotations\n",
    "data = load_annotations(image_folder, annotation_file)\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Display dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'bbox', 'category'],\n",
      "        num_rows: 1637\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'bbox', 'category'],\n",
      "        num_rows: 410\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Optionally split into train and validation sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Show dataset details\n",
    "print(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./TSRD_Train/022_00013_00017_png_jpg.rf.114ef7256b69789e2d952428244509d0.jpg\n"
     ]
    }
   ],
   "source": [
    "print(train_test_split['train'][\"image_path\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image\n",
    "image = Image.open(train_test_split['train'][\"image_path\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0    1    2   3   4    5    6   7\n",
      "0    022_0001.png  210  197  24  24  189  186  22\n",
      "1    022_0002.png   93   85  12  12   79   77  22\n",
      "2    022_0003.png  200  196  28  20  190  182  22\n",
      "3    022_0004.png  179  173  30  27  159  156  22\n",
      "4  022_0001_j.png  166  159  38  32  125  123  22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faac77e82d0c429f82826a4085c6f461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87d881451924bd88e2bc0a05ac3c8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea411cc3dc84f8da11efbd4e2b8f8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def preprocess_data(example):\n",
    "    # Load the image\n",
    "    image = Image.open(example['image_path'])\n",
    "    \n",
    "    # Convert the image to RGB\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    encoding = processor(images=image, annotations=[{\n",
    "        'bbox': example['bbox'],\n",
    "        'category_id': example['category']\n",
    "    }], return_tensors=\"pt\")\n",
    "    \n",
    "    # Add the image and annotations back into the example\n",
    "    example['pixel_values'] = encoding['pixel_values']\n",
    "    example['labels'] = encoding['labels']\n",
    "    example['bbox'] = encoding['bbox']\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Example usage\n",
    "image_folder = './TSRD_Train'  # Replace with your image folder path\n",
    "annotation_file = './TSRD_Train_Annotation/TsignRecgTrain4170Annotation.txt'  # Replace with your annotation file path\n",
    "\n",
    "data = load_annotations(image_folder, annotation_file)\n",
    "\n",
    "# Convert the data to a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Apply the preprocessing\n",
    "dataset = dataset.map(preprocess_data, batched=False)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Apply the preprocessing to the train and test sets\n",
    "train_dataset = train_test_split['train'].map(preprocess_data, batched=False)\n",
    "val_dataset = train_test_split['test'].map(preprocess_data, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [image_path, bbox, annotations, image, category]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2191\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 2193\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[0;32m   2195\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:993\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    991\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m--> 993\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\trainer.py:919\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[1;34m(self, dataset, description)\u001b[0m\n\u001b[0;32m    917\u001b[0m columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names]\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns in the dataset match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms forward method signature. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(datasets\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    926\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39mcolumns, format_kwargs\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    928\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [image_path, bbox, annotations, image, category]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "from transformers import DetrForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load a pre-trained DETR model\n",
    "model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
